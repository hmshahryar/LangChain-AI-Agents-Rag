{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de37fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive character text splitting \n",
    "# line change \\n \n",
    "# \\n\\n para\n",
    "\n",
    "text =\"\"\"Artificial Intelligence (AI) has rapidly evolved from a futuristic concept to a transformative force deeply integrated into modern life. Initially conceived in the mid-20th century as a theoretical field, AI was aimed at enabling machines to mimic human intelligence. Early AI systems were rule-based and limited in capability, often confined to narrow tasks such as playing chess or solving equations. However, with advances in computing power, access to large data sets, and sophisticated algorithms, AI has experienced exponential growth and widespread adoption.\n",
    "\n",
    "Today, AI influences nearly every sector of society. In healthcare, AI-powered diagnostics are revolutionizing patient care. Algorithms can analyze medical images, predict disease outbreaks, and even suggest personalized treatments with greater accuracy and speed than human practitioners. AI chatbots are assisting patients in scheduling appointments and answering health-related queries, reducing the burden on healthcare staff.\n",
    "\n",
    "In the field of education, AI tools personalize learning experiences by adapting content to a studentâ€™s pace and understanding. Intelligent tutoring systems provide real-time feedback and suggest improvements, making education more interactive and effective. Moreover, AI-driven language models help bridge communication gaps by providing instant translations and grammar corrections.\n",
    "\n",
    "The financial sector has also embraced AI to detect fraud, automate trading, and assess creditworthiness. Machine learning models analyze massive volumes of transaction data to flag anomalies and prevent fraudulent activities. Robo-advisors, powered by AI, provide investment guidance to individuals based on their financial goals and risk tolerance.\n",
    "\n",
    "Transportation is another domain where AI has made significant strides. Self-driving vehicles, though still under development, rely heavily on AI to interpret sensory data, make real-time decisions, and navigate safely. Ride-sharing platforms use AI to optimize routes, reduce wait times, and ensure efficient matching of drivers with passengers. In aviation and logistics, AI systems enhance safety, efficiency, and predictive maintenance.\n",
    "\n",
    "Despite its benefits, AI also raises several ethical, social, and economic concerns. One of the most pressing issues is the potential for job displacement due to automation. While AI creates new opportunities in data science and machine learning, it also threatens routine, manual, and even some cognitive jobs. This shift necessitates re-skilling and educational reform to prepare the workforce for AI-driven industries.\n",
    "\n",
    "Bias in AI algorithms is another concern. Since AI systems learn from historical data, they may inherit and even amplify existing societal biases. Ensuring fairness, transparency, and accountability in AI decision-making is crucial to prevent discrimination in areas such as hiring, lending, and law enforcement.\n",
    "\n",
    "Moreover, the rise of AI has sparked debates about data privacy and surveillance. Intelligent systems collect and analyze personal data to deliver personalized experiences, but without proper regulation, this can lead to misuse and erosion of individual privacy.\n",
    "\n",
    "In conclusion, AI is undeniably reshaping the way we live, work, and interact. Its impact is far-reaching and multifacetedâ€”enhancing productivity, unlocking innovation, and offering new solutions to age-old problems. However, the path forward requires a balanced approach: embracing the benefits while addressing the challenges through thoughtful policies, inclusive development, and ethical use. The future of AI holds immense promise, and with responsible stewardship, it can be a force for positive and sustainable change in society.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96baf7c",
   "metadata": {},
   "source": [
    "Resurcive Text Splitter\n",
    "\n",
    "--Good to brak on the basics of sentence good for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d23eace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) has rapidly evolved from a futuristic concept to a transformative force'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# CHUNCK SIZE Allowed is the size is 10 so \n",
    "# it break into para then fulstops then to should be low than the \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100 ,\n",
    "    chunk_overlap = 10\n",
    ")\n",
    "chunk = splitter.split_text(text)\n",
    "chunk[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de1b03",
   "metadata": {},
   "source": [
    "Code spliting using recusive text spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9817786c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Book:\\n    def __init__(self, title, author, isbn, publication_year):'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = \"\"\"import datetime\n",
    "\n",
    "class Book:\n",
    "    def __init__(self, title, author, isbn, publication_year):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.isbn = isbn\n",
    "        self.publication_year = publication_year\n",
    "\n",
    "    def display_info(self):\n",
    "        print(f\"Title: {self.title}\")\n",
    "        print(f\"Author: {self.author}\")\n",
    "        print(f\"ISBN: {self.isbn}\")\n",
    "        print(f\"Publication Year: {self.publication_year}\")\n",
    "\n",
    "    def is_recent_publication(self, years_threshold=5):\n",
    "        current_year = datetime.datetime.now().year\n",
    "        return (current_year - self.publication_year) <= years_threshold\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\"{self.title}\" by {self.author} ({self.publication_year})'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    book1 = Book(\"The Great Gatsby\", \"F. Scott Fitzgerald\", \"978-0743273565\", 1925)\n",
    "    book2 = Book(\"Python Crash Course\", \"Eric Matthes\", \"978-1593279288\", 2019)\n",
    "    book3 = Book(\"Dune\", \"Frank Herbert\", \"978-0441172719\", 1965)\n",
    "    book4 = Book(\"Clean Code\", \"Robert C. Martin\", \"978-0132350884\", 2008)\n",
    "\n",
    "    print(\"--- Book 1 Information ---\")\n",
    "    book1.display_info()\n",
    "    print(f\"Is recent publication? {book1.is_recent_publication()}\")\n",
    "    print(f\"String representation: {book1}\\n\")\n",
    "\n",
    "    print(\"--- Book 2 Information ---\")\n",
    "    book2.display_info()\n",
    "    print(f\"Is recent publication (last 5 years)? {book2.is_recent_publication()}\")\n",
    "    print(f\"Is recent publication (last 10 years)? {book2.is_recent_publication(10)}\\n\")\n",
    "\n",
    "    print(\"--- Book 3 Information ---\")\n",
    "    print(book3)\n",
    "    print(f\"Is recent publication? {book3.is_recent_publication()}\\n\")\n",
    "\n",
    "    print(\"--- Book 4 Information ---\")\n",
    "    print(book4)\n",
    "    print(f\"Is recent publication? {book4.is_recent_publication()}\")\n",
    "\"\"\"\n",
    "\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, \n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "result = splitter.split_text(code)\n",
    "result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa03e4",
   "metadata": {},
   "source": [
    "Sementic Based distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d252dc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MUHAMMAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¾ Split Sentences:\n",
      "1. \n",
      "The farmer used AI to monitor crop yields, while also coaching his village football team using data analytics.\n",
      "0\n",
      "this is s : \n",
      "The farmer used AI to monitor crop yields, while also coaching his village football team using data analytics.\n",
      "2. His cows grazed peacefully as he reviewed drone footage.\n",
      "1\n",
      "this is s : His cows grazed peacefully as he reviewed drone footage.\n",
      "3. The AI assistant alerted him about soil dryness and also showed replays of yesterdayâ€™s game.\n",
      "2\n",
      "this is s : The AI assistant alerted him about soil dryness and also showed replays of yesterdayâ€™s game.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. check_pairwise_arrays expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m current_chunk \u001b[38;5;241m=\u001b[39m [sentences[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences)):\n\u001b[1;32m---> 37\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msentence_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_chunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[0;32m     43\u001b[0m         current_chunk\u001b[38;5;241m.\u001b[39mappend(sentences[i])\n",
      "File \u001b[1;32mc:\\Users\\MUHAMMAD\\miniconda3\\envs\\DS-ML\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MUHAMMAD\\miniconda3\\envs\\DS-ML\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\MUHAMMAD\\miniconda3\\envs\\DS-ML\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:209\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[1;32m--> 209\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\MUHAMMAD\\miniconda3\\envs\\DS-ML\\lib\\site-packages\\sklearn\\utils\\validation.py:1101\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1099\u001b[0m     )\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m   1107\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1108\u001b[0m         array,\n\u001b[0;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1112\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Sample multi-domain paragraph\n",
    "text = \"\"\"\n",
    "The farmer used AI to monitor crop yields, while also coaching his village football team using data analytics.\n",
    "His cows grazed peacefully as he reviewed drone footage.\n",
    "The AI assistant alerted him about soil dryness and also showed replays of yesterdayâ€™s game.\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Split into sentences or semantic phrases\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"\\nðŸ§¾ Split Sentences:\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"{i+1}. {s}\")\n",
    "    print(i)\n",
    "    print('this is s :',s)\n",
    "\n",
    "# Step 3: Embed sentences using sentence transformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "# Step 4: Compare sentence similarities for semantic chunking\n",
    "threshold = 0.65\n",
    "chunks = []\n",
    "current_chunk = [sentences[0]]\n",
    "\n",
    "for i in range(1, len(sentences)):\n",
    "    sim = cosine_similarity(\n",
    "        [sentence_embeddings[i]], \n",
    "        [model.encode([\" \".join(current_chunk)])]\n",
    "    )[0][0]\n",
    "    \n",
    "    if sim >= threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sentences[i]]\n",
    "\n",
    "# Add the last chunk\n",
    "if current_chunk:\n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "print(\"\\nðŸ§© Semantic Chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
